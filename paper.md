---
title: 'RHEA: an open-source Reproducible Hybrid-architecture flow solver Engineered for Academia'
tags:
  - Compressible flow
  - Fluid mechanics
  - Ideal- & real-gas thermodynamics
  - Turbulence
  - C++
  - MPI
  - OpenACC
  - HDF5
  - YAML
authors:
  - name: Lluís Jofre
    orcid: 0000-0003-2437-259X
    affiliation: 1
  - name: Ahmed Abdellatif
    orcid: 0000-0002-8810-7480
    affiliation: 1
  - name: Guillermo Oyarzun
    orcid: 0000-0001-9524-3782
    affiliation: 2
affiliations:
 - name: Dept. Fluid Mechanics, Technical University of Catalonia - BarcelonaTech, Barcelona 08019, Spain
   index: 1
 - name: Dept. Computer Applications in Science & Engineering, Barcelona Supercomputing Center, Barcelona 08034, Spain
   index: 2
date: 11 July 2022
bibliography: paper.bib
---

# Summary

Fluid mechanics systems presenting coupled processes active at multiple scales are encountered in a plethora of fundamental and applied problems, like for example the motion of small-scale (Kolmogorov-size) turbulent eddies over large aerodynamic structures [@Jofre2022-A], microconfined high-pressure supercritical fluids for enhanced energy transfer [@Bernades2022-A], or hydrodynamic focusing of microorganisms in wall-bounded flows [@Palacios2022-A]. In this regard, the study of microscale flow phenomena and their interaction with larger scales is growing at a rapid pace as it has been recognized that the ability to control fluids at such small scales is leading to advances in basic research and technological innovations [@Groen2014-A]. However, many research challenges arise in such problems: (i) detailed understanding of the flow mechanisms, (ii) how to effectively model and coarse-grain multiphysics flow phenomena, (iii) the creation of validation databases, and (iv) the development of efficient computational frameworks for engineering design and optimization.

The study of complex multiscale flows greatly benefits from the combination of interconnected theoretical, computational and experimental approaches. This manifold methodology provides a robust framework to corroborate the phenomena observed, validate the modeling assumptions utilized, and facilitates the exploration of wider parameter spaces and extraction of more sophisticated insights. These analyses are typically encompassed within the field of Predictive Science & Engineering [@Njam2009-A], which has attracted attention in the Fluid Mechanics community and is expected to exponentially grow as computational studies transition from (mostly) physics simulations to active vectors for scientific discovery and technological innovation with the advent of Exascale computing [@Alowayyed2017-A]. In this regard, the computational flow solver presented aims at bridging the gap between studying complex multiscale fluid mechanics problems and utilizing present and future state of the art supercomputing systems in academic environments. The solver presented is named RHEA, which stands for *open-source **R**eproducible **H**ybrid-architecture flow solver **E**ngineered for **A**cademia*, and is inspired by the Titaness great Mother of the ancient Greek Gods, and goddess of female fertility, motherhood, and generation. Her name was RHEA and means *flow* and *ease*, representing the eternal flow of time and generations with ease. RHEA is available as an open-source Git repository at https://gitlab.com/ProjectRHEA/flowsolverrhea.

# Statement of need

/*******************/
In recent years, genetic analysis in autopolyploid crops has been gaining attention due to the flourishing of high-throughput genotyping technologies, which deliver massive amounts of DNA sequences. This technological advance prompted the development of many computational tools to perform an in-depth analysis of these complex genomes. Programs such as `TetraploidSNPmap` [@Hackett2017], `polymapR` [@Bourke2018], and `MAPpoly` [@Mollinari2019; @Mollinari2020] use dosage-based markers to build genetic maps and obtain the linkage phases in full-sib mapping populations. `TetraOrigin` [@Zheng2016], `PolyOrigin` [@Zheng2021], `MAPpoly` [@Mollinari2019], and `polyqtlR` [@Bourke2021] provide the probabilistic haplotype inheritance profiles of individuals in mapping populations in terms of their respective parental haplotypes. These probabilities are used to estimate the position and genetic effects of quantitative trait loci (QTL). Using this information, researchers can benefit from the inferred genome-wide genotype and phenotype relationship to make informed breeding decisions. Software programs for performing such analysis in autopolyploids include `GWASpoly` [@Rosyara2016], `QTLpoly` [@Pereira2020], `polyqtlR` [@Bourke2021], and `diaQTL` [@Amadeu2021]. Despite the increasing availability of computational and analytical tools focused on autopolyploid species, the need for programming skills to comprehensively organize, integrate, display results of these programs still imposes an obstacle to exploring and using this information in practical situations. We present `VIEWpoly`, a user-friendly R package that allows easy integration, visualization, and exploration of upstream genetic and genomic analysis focused on polyploid organisms. `VIEWpoly` is a self-contained interactive R package built as `Shiny` modules using `golem` [@Fay2022] framework. It includes documentation, data examples, and comprehensive video tutorials to guide users in easy-to-follow steps to extract information from polyploid computational tools in a seamless manner. It can be deployed locally or on servers.

The ever-increasing demand for resolution and accuracy in mathematical models of physical processes governed by systems of Partial Differential Equations (PDEs) can only be addressed using fully-parallel advanced numerical discretization methods and scalable solution methods, thus able to exploit the vast amount of computational resources in state-of-the-art supercom- puters. To this end, GridapDistributed is a registered Julia (Bezanson et al., 2017) software package which provides fully-parallel distributed memory data structures and associated meth- ods for the Finite Element (FE) numerical solution of PDEs on parallel computers. Thus, it can be run on multi-core CPU desktop computers at small scales, as well as on HPC clusters and supercomputers at medium/large scales. The data structures in GridapDistributed are designed to mirror as far as possible their counterparts in the Gridap (Badia & Verdugo, 2020) Julia software package, while implementing/leveraging most of their abstract interfaces (see Francesc Verdugo & Badia (2022) for a detailed overview of the software design of Gridap). As a result, sequential Julia scripts written in the high-level Application Programming Interface (API) of Gridap can be used verbatim up to minor adjustments in a parallel distributed memory context using GridapDistributed. This equips end-users with a tool for the development of simulation codes able to solve real-world application problems on massively parallel supercomputers while using a highly expressive, compact syntax that resembles mathematical notation. This is indeed one of the main advantages of GridapDistributed and a major design goal that we pursue.

In order to scale FE simulations to large core counts, the mesh used to discretize the com- putational domain on which the PDE is posed must be partitioned (distributed) among the parallel tasks such that each of these only holds a local portion of the global mesh. The same requirement applies to the rest of data structures in the FE simulation pipeline, i.e., FE space, linear system, solvers, data output, etc. The local portion of each task is composed by a set of cells that it owns, i.e., the local cells of the task, and a set of off-processor cells (owned by remote processors) which are in touch with its local cells, i.e., the ghost cells of the task (Badia et al., 2020). This overlapped mesh partition is used by GridapDistributed, among others, to exchange data among nearest neighbors, and to glue together global Degrees of Freedom (DoFs) which are sitting on the interface among subdomains. Following this design principle, GridapDistributed provides scalable parallel data structures and associated methods for simple grid handling (in particular, Cartesian-like meshes of arbitrary-dimensional, topologically n-cube domains), FE spaces setup, and distributed linear system assembly. It is in our future plans to provide highly scalable linear and nonlinear solvers tailored for the FE discretization of PDEs (e.g., linear and nonlinear matrix-free geometric multigrid and domain decomposition preconditioners). In the meantime, however, GridapDistributed can be combined with other Julia packages in order to realize the full potential required in real-world applications. These packages and their relation with GridapDistributed are overviewed in the next section.

There are a number of high quality open source parallel finite element packages available in the literature. Some examples are deal.II (Arndt et al., 2021), libMesh (Kirk et al., 2006), MFEM (Anderson et al., 2021), FEMPAR (Badia et al., 2017), FEniCS (Logg et al., 2012), or FreeFEM++ (Hecht, 2012), to name a few. All these packages have their own set of features, potentials, and limitations. Among these, FEniCS and FreeFEM++ are perhaps the closest ones in scope and spirit to the packages in the Gridap ecosystem. A hallmark of Gridap ecosystem packages compared to FreeFEM++ and FEniCS is that a very expressive and compact (yet efficient) syntax is transformed into low-level code using the Julia JIT compiler and thus they do not need a sophisticated compiler of variational forms nor a more intricate workflow (e.g., a Python front-end and a C/C++ back-end).
/*******************/

# Computational design

The flow solver RHEA solves the compressible equations of fluid motion utilizing non-uniform Cartesian second-order central finite differences [@Moin2010-B] in combination with kinetic energy preserving [@Coppola2019-A] convection schemes, or different Harten-Lax-van-Leer-type (HLL) Riemann solvers [@Toro2009-B], and uses explicit Runge-Kutta methods [@Gottlieb2001-A] for time integration. The set of conservation equations is closed by means of ideal- or real-gas thermodynamics to target, respectively, subcritical and supercritical fluid regimes [@Jofre2021-A]. The solver can be utilized for a wide range of fluid mechanics problems as it allows (i) to easily overwrite most high-level kernels to set, for example, specific initial conditions and source terms, and (ii) select between Dirichlet, Neumann, Periodic, and subsonic & supersonic inflow-outflow [@Poinsot1992-A] boundary conditions. RHEA is written in C++ [@Stroustrup2013-B], using object-oriented programming, utilizes YAML [@YAML] and HDF5 [@HDF5] for input/output operations, and targets hybrid (CPU-GPU) supercomputing architectures based on a state-of-the-art parallel and scalable MPI [@MPI] + OpenACC [@OpenACC] accelerated (managed memory) computational framework.

The computational performance of RHEA is depicted in Figure \autoref{fig:computational_performance} by carrying out (i) time- \& energy-to-solution, and (ii) strong scalability tests based on $100$ time iterations of the 3-D turbulent channel flow configuration described in the next section. In this regard, to assess the portability of RHEA to different computing systems, the performance tests have been performed on (i) a local hybrid machine (results are referred to as Hybrid), and (ii) the Barcelona Supercomputer Center [@BSC] (results are indicated as BSC). The Hybrid machine is composed of a node with 1 AMD Ryzen 9 3900XT 12-core CPU and 2 NVIDIA Quadro RTX 4000 GPUs, while the BSC supercomputer contains a CPU-GPU cluster with 3 racks formed of 54 IBM POWER9 nodes, each containing 2 Witherspoons CPUs (20 cores of 3.1 GHz each), 4 Volta NVIDIA GPUs (16 GB each), and 6.4 TB of NVMe (Non-Volatile Memory). Three main observations can be inferred from the results. First, when using CPUs+GPUs with respect to only CPUs, the solver is accelerated approximately $2.5\times$ and $5\times$ on the Hybrid and BSC computers. Second, running on CPUs+GPUs consume more power (Joules per second, i.e., Watts) than on CPUs, however, the energy-to-solution (Joules) is reduced by factors of roughly $1.3$ and $2.5$ on the Hybrid and BSC systems, respectively. Third, on BSC for a fixed-problem size, the solver presents nearly ideal speedup in terms of strong scalability running on both CPUs and CPUs+GPUs up to 32 nodes (640 cores and 128 GPUs) until communication overheads become important in relative value.

![Ratios of time-to-solution (Time) and energy-to-solution (Energy) of RHEA on Hybrid machine and BSC supercomputer using CPUs+GPUs versus CPUs (left).\label{fig:computational_performance}](time_energy_solution_hybrid_bsc_cpu_gpu.png)
![Strong scalability of RHEA on BSC supercomputer using CPUs and CPUs+GPUs (right).\label{fig:computational_performance}](strong_speedup_bsc_cpu_gpu.png)

# Application example
The ability of RHEA to easily configure computational flow problems and run them efficiently on top-tier supercomputing systems is demonstrated by simulating the canonical three-dimensional (3-D) turbulent channel flow problem [@Smits2011-A] on 2 nodes of the CTE POWER9 cluster of the Barcelona Supercomputing Center [@BSC]. The friction Reynolds number selected is $Re_{\tau} = u_{\tau} \delta / \nu = 180$, where $u_{\tau} = 1\thinspace\textrm{m/s}$ is the friction velocity, $\delta=1\thinspace\textrm{m}$ is the channel half-height, and $\nu = \mu / \rho$ is the kinematic viscosity of the fluid with $\mu$ the dynamic viscosity and $\rho = 1\thinspace\textrm{kg/m}^\textrm{3}$ the density. The Prandtl number of the problem is $Pr = c_p \mu/\kappa = 0.71$ with $c_p$ the isobaric specific heat capacity, and the Mach number is $Ma = u_b/\sqrt{\gamma P_b/\rho} = 0.3$ with $u_{b}$ and $P_b$ the bulk velocity and pressure, respectively, and $\gamma=1.4$ the heat capacity ratio. The mass flow rate in the streamwise direction is imposed through a body force equal to $\textbf{f}=\left[\tau_{w}/\delta,0,0\right]^{\intercal}$, where $\tau_{w}$ is the wall shear stress.

The computational domain is $4 \pi \delta \times 2\delta \times 4/3\pi \delta$ in the streamwise ($x$), wall-normal ($y$), and spanwise ($z$) directions, respectively. The streamwise and spanwise boundaries are set periodic, and no-slip conditions are imposed on the horizontal boundaries ($x$-$z$ planes). The grid is uniform in the streamwise and spanwise directions with resolutions in wall units equal to $\Delta x^{+} = 9$ and $\Delta z^{+} = 6$, and stretched toward the walls in the vertical direction with the first grid point at $y^{+} = y u_{\tau}/\nu =0.1$ and with sizes in the range $0.1 \lesssim \Delta y^{+} \lesssim 4$. This grid arrangement corresponds to a direct numerical simulation (DNS) of size $256 \times 128 \times 128$ grid points. The simulation strategy starts from a linear velocity profile with random fluctuations [@Nelson2017-A], which is advanced in time utilizing the KGP convection scheme [@Coppola2019-A] with $\textrm{CFL}=0.9$ to reach turbulent steady-state conditions after approximately $20$ flow-through-time (FTT) units; based on the bulk velocity $u_{b}$ and the length of the channel $L_x = 4\pi\delta$, a FTT is defined as $t_{b} = L_x/u_{b} \sim \delta/u_{\tau}$. Flow statistics are collected for roughly $30$ FTTs once steady-state conditions are achieved, and compared against reference results [@Moser1999-A].

![Comparison against reference results [@Moser1999-A] of the time-averaged streamwise velocity $u^+$ (left) and $\textrm{rms}$ velocity fluctuations $u_{\textrm{rms}}^{+}$, $v_{\textrm{rms}}^{+}$ and $w_{\textrm{rms}}^{+}$ (right) along the wall-normal direction $y^+$ in wall units.\label{fig:3d_turbulent_channel_flow_statistics}](u_plus_vs_y_plus.png)
![Comparison against reference results [@Moser1999-A] of the time-averaged streamwise velocity $u^+$ (left) and $\textrm{rms}$ velocity fluctuations $u_{\textrm{rms}}^{+}$, $v_{\textrm{rms}}^{+}$ and $w_{\textrm{rms}}^{+}$ (right) along the wall-normal direction $y^+$ in wall units.\label{fig:3d_turbulent_channel_flow_statistics}](uvw_rms_plus_vs_y_plus.png)

The time-averaged mean streamwise velocity $u^+$ and root-mean-squared ($\textrm{rms}$) velocity fluctuations $u_{\textrm{rms}}^{+}$, $v_{\textrm{rms}}^{+}$, $w_{\textrm{rms}}^{+}$ along the wall-normal direction $y^+$ in wall units provided by RHEA and compared to reference results [@Moser1999-A] are depicted in Figure \autoref{fig:3d_turbulent_channel_flow_statistics}. As shown in the figure, the results from RHEA accurately reproduce the first- and second-order turbulent flow statistics by (i) properly capturing the inner (viscous sublayer, buffer layer and and log-law region) and outer layers, and (ii) the turbulent flow fluctuations peaking around $y^+\approx 15$ for the streamwise velocity. Additionally, a snapshot of the instantaneous streamwise velocity in wall units $u^+$ on a $x^+$-$y^+$ slice is displayed in Figure \autoref{fig:3d_turbulent_channel_flow_contours} to provide qualitative information of the wall-bounded turbulence at $Re_\tau = 180$ computed by RHEA.

![Snapshot of the instantaneous streamwise velocity in wall units $u^+$ on a $x^+$-$y^+$ slice from RHEA.\label{fig:3d_turbulent_channel_flow_contours}](channel_u_velocity_vs_x_y_direction.png)

# Acknowledgements
This work is supported by the European Research Council (ERC) under the European Union’s Horizon Europe research and innovation programme (grant agreement No. 101040379 - SCRAMBLE), and the *Beatriz Galindo* programme (Distinguished Researcher, BGP18/00026) of the *Ministerio de Ciencia, Innovación y Universidades* (Spain).

# References
